{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFrMkoBMv-Bw"
      },
      "source": [
        "## Basic BERT Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS1-aIhwtn8u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.nn.functional import softmax\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('cleaned_data.csv')\n",
        "\n",
        "# Tokenize text and encode labels\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "encoded_data = tokenizer(df['combined_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "labels = torch.tensor(df['combined_category'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(encoded_data['input_ids'],\n",
        "                                                                     labels,\n",
        "                                                                     test_size=0.2,\n",
        "                                                                     random_state=42)\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = TensorDataset(train_texts, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "# Create DataLoader for testing\n",
        "test_dataset = TensorDataset(test_texts, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "sNwKJ-atT2-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['combined_category'].unique()))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "29kceSymT7Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"Average Epoch Loss: {avg_epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "7UeCrKF6T-R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids=inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the model to a file\n",
        "model.save_pretrained('bert_model')\n",
        "tokenizer.save_pretrained('bert_model')"
      ],
      "metadata": {
        "id": "pJIMyDLAUAnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the file\n",
        "loaded_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['combined_category'].unique()))\n",
        "loaded_model.load_state_dict(torch.load('bert_model/pth'))\n",
        "loaded_model.eval()  # Set the model to evaluation model\n",
        "\n",
        "# Example text for evaluation\n",
        "text = \"I'm concerned about not remembering stuff from calc\"\n",
        "\n",
        "# Tokenize and encode the text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(**inputs)\n",
        "\n",
        "# Get predicted probabilities\n",
        "probs = softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Class probabilities: {probs.tolist()}\")"
      ],
      "metadata": {
        "id": "1LyHFhKXUEBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}