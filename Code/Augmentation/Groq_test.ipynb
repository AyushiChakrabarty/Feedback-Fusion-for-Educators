{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"GROQ_API_KEY\"] = \"\"\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "#print(f\"GROQ_API_KEY: {api_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as efficient language models or Accelerated Language Models, have gained significant attention in recent years due to their importance in various applications. Here are some reasons why fast language models are crucial:\n",
      "\n",
      "1. **Real-time processing**: Fast language models enable real-time processing of natural language data, making them essential for applications like chatbots, voice assistants, and voice-to-text systems. These systems require immediate responses to user inputs, and fast language models can provide them quickly.\n",
      "2. **Efficient inference**: Traditional language models are computationally expensive, which can lead to slow inference times. Fast language models, on the other hand, are designed to have faster inference times, making them suitable for applications where speed and efficiency are critical, such as search engines and recommendation systems.\n",
      "3. **Scalability**: As the amount of user-generated content grows, traditional language models may become impractical due to their computational requirements. Fast language models can handle massive datasets and scale better, allowing them to process large volumes of data in a shorter time.\n",
      "4. **Edge AI**: Fast language models are essential for Edge AI applications, where devices like smartphones, smart home devices, and IoT devices need to process data locally. These models can run on lower-power processors and reduce the need for cloud-based processing.\n",
      "5. **Cross-domain adaptation**: Fast language models can be adapted to various domains and tasks, enabling applications like language translation, sentiment analysis, and text summarization. These models can be fine-tuned for specific domains, reducing the need for retraining from scratch.\n",
      "6. **Quantization and deployment**: Fast language models can be quantized to binary or low-precision formats, making them suitable for deployment on resource-constrained devices. This reduces memory requirements and energy consumption, making them more practical for real-world applications.\n",
      "7. **Knowledge graph embedding**: Fast language models can be used to embed knowledge graphs, which are crucial for applications like question answering, recommender systems, and expert systems. These models can learn to represent entities and relationships within the knowledge graph efficiently.\n",
      "8. **Explainability and transparency**: Fast language models can provide insights into the reasoning behind their predictions, making them more explainable and transparent. This is essential for applications where trust and transparency are critical, such as in the medical or legal fields.\n",
      "9. **Few-shot learning**: Fast language models can be used for few-shot learning, where the model is trained on a small set of examples and then applied to new, unseen data. This is useful for applications like zero-shot learning and out-of-distribution generalization.\n",
      "10. **Advancements in AI research**: Fast language models have the potential to accelerate AI research by enabling researchers to explore new ideas and techniques without being limited by computational resources. This can lead to breakthroughs in areas like natural language processing, computer vision, and speech recognition.\n",
      "\n",
      "In summary, fast language models are crucial for a wide range of applications that require efficient, scalable, and adaptable language processing capabilities. Their importance extends beyond traditional NLP tasks to various domains, including cross-domain adaptation, edge AI, and knowledge graph embedding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Low latency is crucial for Large Language Models (LLMs) because it directly impacts the user experience, model performance, and overall efficiency of language-based applications. Here are some reasons why low latency is essential for LLMs:\\n\\n1. **Real-time Interaction**: LLMs are often used in applications that require real-time interaction, such as chatbots, virtual assistants, and language translation systems. Low latency ensures that the model responds quickly to user input, providing a seamless and engaging experience.\\n2. **Conversational Flow**: In conversational AI, latency can disrupt the natural flow of conversation. High latency can lead to awkward pauses, making the interaction feel unnatural and frustrating. Low latency helps maintain a smooth conversation, allowing users to engage more naturally with the model.\\n3. **User Engagement**: High latency can lead to user frustration, causing them to abandon the application or lose interest. Low latency, on the other hand, keeps users engaged, increasing the chances of successful task completion and encouraging them to continue interacting with the model.\\n4. **Model Performance**: Latency can affect the performance of LLMs, particularly in scenarios where the model needs to process sequential data, such as in language translation or text generation. High latency can lead to errors, inaccuracies, or incomplete responses, while low latency enables the model to process data more efficiently and accurately.\\n5. **Scalability**: As LLMs are deployed in large-scale applications, low latency becomes critical to handle a high volume of requests without sacrificing performance. Scalable architectures with low latency enable LLMs to handle increased traffic, ensuring consistent performance and reliability.\\n6. **Resource Efficiency**: Low latency can lead to more efficient resource utilization, as the model can process requests quickly and free up resources for other tasks. This is particularly important in cloud-based deployments, where resource efficiency can lead to cost savings and improved overall performance.\\n7. **Competitive Advantage**: In competitive markets, low latency can be a key differentiator for LLM-based applications. Faster response times can lead to increased user adoption, retention, and ultimately, revenue growth.\\n8. **Edge Computing**: With the rise of edge computing, LLMs are being deployed closer to the user, reducing latency and improving real-time processing capabilities. Low latency is essential in edge computing scenarios, where data processing occurs at the edge of the network, rather than in a centralized cloud or data center.\\n9. **Safety-Critical Applications**: In safety-critical applications, such as autonomous vehicles or medical diagnosis, low latency is crucial to ensure timely and accurate decision-making. High latency can have severe consequences, making low latency a critical requirement for these applications.\\n10. **Future-Proofing**: As LLMs continue to evolve and become more pervasive in various industries, low latency will become increasingly important to support emerging use cases, such as augmented reality, virtual reality, and other real-time applications.\\n\\nIn summary, low latency is vital for LLMs to provide a seamless user experience, maintain conversational flow, and ensure efficient resource utilization. It is essential for achieving high performance, scalability, and reliability in a wide range of applications, from chatbots and virtual assistants to safety-critical systems and edge computing deployments.', response_metadata={'token_usage': {'completion_tokens': 651, 'prompt_tokens': 33, 'total_tokens': 684, 'completion_time': 1.768317504, 'prompt_time': 0.007803656, 'queue_time': None, 'total_time': 1.7761211600000002}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_abd29e8833', 'finish_reason': 'stop', 'logprobs': None}, id='run-274cb278-7f5b-4131-b00a-38853fc173f7-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    # api_key=\"\" # Optional if not set as an environment variable\n",
    ")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency for LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_weather',\n",
       "  'args': {'location': 'San Francisco', 'unit': 'Celsius'},\n",
       "  'id': 'call_mh86'},\n",
       " {'name': 'get_current_weather',\n",
       "  'args': {'location': 'Tokyo', 'unit': 'Celsius'},\n",
       "  'id': 'call_yexd'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_weather(location: str, unit: Optional[str]):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return \"Cloudy with a chance of rain.\"\n",
    "\n",
    "\n",
    "tool_model = chat.bind_tools([get_current_weather], tool_choice=\"auto\")\n",
    "\n",
    "res = tool_model.invoke(\"What is the weather like in San Francisco and Tokyo?\")\n",
    "\n",
    "res.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!', rating=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a limerick about the sun:\\n\\nThere once was a sun in the sky,\\nWhose warmth and light caught the eye.\\nIt shone bright and bold,\\nWith a heat that was told,\\nAnd brought life to all, as it flew by.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 18, 'total_tokens': 72, 'completion_time': 0.144831532, 'prompt_time': 0.005151873, 'queue_time': None, 'total_time': 0.14998340500000001}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None}, id='run-cb464bc3-7a3b-4a17-8916-cb8952d6301f-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a Limerick about {topic}\")])\n",
    "chain = prompt | chat\n",
    "await chain.ainvoke({\"topic\": \"The Sun\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver glowing face\n",
      "Luna's gentle light descends\n",
      "Midnight's peaceful hush"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\"response\": \"That\\'s a tough question! There are eight species of bears found in the world, and each has its own unique characteristics. However, if I had to pick one, I\\'d say the giant panda is a popular favorite due to its distinct black and white markings and gentle nature.\", \"followup_question\": \"What do you think makes a bear \\'best\\' - its size, its habitat, or something else? \"}', response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 50, 'total_tokens': 139, 'completion_time': 0.244468874, 'prompt_time': 0.011064902, 'queue_time': None, 'total_time': 0.255533776}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None}, id='run-157fd267-4523-450f-bb78-c0dbda683d68-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\", model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Always respond with a JSON object with two string keys: \"response\" and \"followup_question\".\n",
    "\"\"\"\n",
    "human = \"{question}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"question\": \"what bear is best?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91963\\AppData\\Local\\Temp\\ipykernel_36472\\1145404251.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['concerns'] = df['concerns'].apply(clean_text)\n",
      "C:\\Users\\91963\\AppData\\Local\\Temp\\ipykernel_36472\\1145404251.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anything else'] = df['anything else'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data saved to ../../Data/augmented_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set your API key (ensure you have it set in your environment)\n",
    "#os.environ['GROQ_API_KEY'] = 'your_api_key_here'\n",
    "\n",
    "# Initialize the Groq chat model\n",
    "chat = ChatGroq(\n",
    "    temperature=0.7,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key=os.getenv('GROQ_API_KEY')\n",
    ")\n",
    "\n",
    "def sanitize_data(df):\n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna(subset=['concerns', 'anything else'])\n",
    "    \n",
    "    # Function to clean text by removing special characters and leading/trailing whitespaces\n",
    "    def clean_text(text):\n",
    "        # Remove leading/trailing whitespaces\n",
    "        text = text.strip()\n",
    "        # Remove special characters (except common punctuation and spaces)\n",
    "        text = re.sub(r'[^A-Za-z0-9.,?!\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    # Apply cleaning function to 'concerns' and 'anything else' columns\n",
    "    df['concerns'] = df['concerns'].apply(clean_text)\n",
    "    df['anything else'] = df['anything else'].apply(clean_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = '../../Data/Merged_data.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "data = sanitize_data(data)\n",
    "\n",
    "# Define the prompt template\n",
    "system = \"You are a helpful assistant.\"\n",
    "human_template = \"Augment this response: {text}\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human_template)])\n",
    "\n",
    "# Function to generate augmented data\n",
    "def augment_text(text):\n",
    "    chain = prompt | chat\n",
    "    augmented_response = chain.invoke({\"text\": text})\n",
    "    return augmented_response.content\n",
    "\n",
    "# Generate augmented data for each row\n",
    "augmented_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    augmented_concerns = augment_text(row['concerns'])\n",
    "    augmented_anything_else = augment_text(row['anything else'])\n",
    "    \n",
    "    augmented_data.append({\n",
    "        'response id': row['response id'],\n",
    "        'concerns': augmented_concerns,\n",
    "        'concerns category': row['concerns category'],\n",
    "        'anything else': augmented_anything_else,\n",
    "        'anything else category': row['anything else category']\n",
    "    })\n",
    "\n",
    "# Convert augmented data to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Save augmented data to a new CSV file\n",
    "output_file = '../../Data/augmented_data.csv'\n",
    "augmented_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Augmented data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91963\\AppData\\Local\\Temp\\ipykernel_36472\\2855982443.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['concerns'] = df['concerns'].apply(clean_text)\n",
      "C:\\Users\\91963\\AppData\\Local\\Temp\\ipykernel_36472\\2855982443.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anything else'] = df['anything else'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data saved to ../../Data/synthetic_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import re\n",
    "\n",
    "# Set your API key (ensure you have it set in your environment)\n",
    "#os.environ['GROQ_API_KEY'] = 'your_api_key_here'\n",
    "\n",
    "# Initialize the Groq chat model\n",
    "chat = ChatGroq(\n",
    "    temperature=0.7,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key=os.getenv('GROQ_API_KEY')\n",
    ")\n",
    "\n",
    "def sanitize_data(df):\n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna(subset=['concerns', 'anything else'])\n",
    "    \n",
    "    # Function to clean text by removing special characters and leading/trailing whitespaces\n",
    "    def clean_text(text):\n",
    "        # Remove leading/trailing whitespaces\n",
    "        text = text.strip()\n",
    "        # Remove special characters (except common punctuation and spaces)\n",
    "        text = re.sub(r'[^A-Za-z0-9.,?!\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    # Apply cleaning function to 'concerns' and 'anything else' columns\n",
    "    df['concerns'] = df['concerns'].apply(clean_text)\n",
    "    df['anything else'] = df['anything else'].apply(clean_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = '../../Data/Merged_data.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "data = sanitize_data(data)\n",
    "\n",
    "# Define the prompt template for generating synthetic data\n",
    "system = \"You are a helpful assistant that generates realistic synthetic data for a survey.\"\n",
    "human_template = \"Generate a synthetic response for the concerns category: {concerns_category} and anything else category: {anything_else_category}.\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human_template)])\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_synthetic_data(concerns_category, anything_else_category):\n",
    "    chain = prompt | chat\n",
    "    synthetic_response = chain.invoke({\n",
    "        \"concerns_category\": concerns_category,\n",
    "        \"anything_else_category\": anything_else_category\n",
    "    })\n",
    "    return synthetic_response.content\n",
    "\n",
    "# Generate synthetic data for each row\n",
    "synthetic_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    synthetic_response = generate_synthetic_data(row['concerns category'], row['anything else category'])\n",
    "    synthetic_concerns, synthetic_anything_else = synthetic_response.split('\\n', 1)\n",
    "    \n",
    "    synthetic_data.append({\n",
    "        'response id': row['response id'],\n",
    "        'concerns': synthetic_concerns.strip(),\n",
    "        'concerns category': row['concerns category'],\n",
    "        'anything else': synthetic_anything_else.strip(),\n",
    "        'anything else category': row['anything else category']\n",
    "    })\n",
    "\n",
    "# Convert synthetic data to DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Save synthetic data to a new CSV file\n",
    "output_file = '../../Data/synthetic_data.csv'\n",
    "synthetic_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Synthetic data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91963\\AppData\\Local\\Temp\\ipykernel_36472\\1680040669.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['concerns'] = df['concerns'].apply(clean_text)\n",
      "C:\\Users\\91963\\AppData\\Local\\Temp\\ipykernel_36472\\1680040669.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['anything else'] = df['anything else'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data saved to ../../Data/synthetic_data_final.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import re\n",
    "\n",
    "# Set your API key (ensure you have it set in your environment)\n",
    "#os.environ['GROQ_API_KEY'] = 'your_api_key_here'\n",
    "\n",
    "# Initialize the Groq chat model\n",
    "chat = ChatGroq(\n",
    "    temperature=0.7,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key=os.getenv('GROQ_API_KEY')\n",
    ")\n",
    "\n",
    "def sanitize_data(df):\n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna(subset=['concerns', 'anything else'])\n",
    "    \n",
    "    # Function to clean text by removing special characters and leading/trailing whitespaces\n",
    "    def clean_text(text):\n",
    "        # Remove leading/trailing whitespaces\n",
    "        text = text.strip()\n",
    "        # Remove special characters (except common punctuation and spaces)\n",
    "        text = re.sub(r'[^A-Za-z0-9.,?!\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    # Apply cleaning function to 'concerns' and 'anything else' columns\n",
    "    df['concerns'] = df['concerns'].apply(clean_text)\n",
    "    df['anything else'] = df['anything else'].apply(clean_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = '../../Data/Merged_data.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "data = sanitize_data(data)\n",
    "\n",
    "# Define the prompt template for generating synthetic data\n",
    "system = \"You are a helpful assistant that generates realistic synthetic data for a survey. Generate responses that are similar in style and context to the provided examples.\"\n",
    "human_template = \"Given the following categories: \\nConcerns Category: {concerns_category}\\nAnything Else Category: {anything_else_category} \\\n",
    "\\nHere, AC stands for Academic Concerns, TC for Technical Concerns, PC for Personal Concerns, and NC for No Concerns. The {concerns} and the {anything_else}\\\n",
    "represent open-ended responses entered by students after participating in the start-of-semester survey. {concerns_category} label corresponds to {concerns} and {anything_else_category} label corresponds to {anything_else}.This survey is for a distance Multivariable Calculus course.\\\n",
    "\\nGenerate a synthetic response similar to the examples but covering more real-world variations.\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human_template)])\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_synthetic_data(concerns, anything_else, concerns_category, anything_else_category):\n",
    "    chain = prompt | chat\n",
    "    synthetic_response = chain.invoke({\n",
    "        \"concerns\": concerns,\n",
    "        \"anything_else\": anything_else,\n",
    "        \"concerns_category\": concerns_category,\n",
    "        \"anything_else_category\": anything_else_category\n",
    "    })\n",
    "    return synthetic_response.content\n",
    "\n",
    "# Generate synthetic data for each row\n",
    "synthetic_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # Generate synthetic response for concerns\n",
    "    synthetic_concerns = generate_synthetic_data(row['concerns category'], row['anything else category'], row['concerns'], row['anything else'])\n",
    "    \n",
    "    synthetic_data.append({\n",
    "        'response id': f'synthetic_{index}',\n",
    "        'concerns': synthetic_concerns,\n",
    "        'concerns category': row['concerns category'],\n",
    "        'anything else': synthetic_concerns,\n",
    "        'anything else category': row['anything else category']\n",
    "    })\n",
    "\n",
    "# Convert synthetic data to DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Save synthetic data to a new CSV file\n",
    "output_file = '../../Data/synthetic_data_final.csv'\n",
    "synthetic_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Synthetic data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../Data/After/2023.02.StartOfSemester.Coded.csv'  # Update with your file path\n",
    "# Try different encodings if the default 'utf-8' fails\n",
    "encodings = ['latin1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"File successfully read with encoding: {encoding}\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read file with encoding: {encoding}\")\n",
    "\n",
    "# Select the columns to be augmented\n",
    "columns_to_generate = ['concerns', 'anything else']\n",
    "\n",
    "def call_groq_api_for_generation(prompt, api_key):\n",
    "    url = \"https://api.groq.com/generate\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_length\": 200  # Adjust based on your requirements\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"generated_text\"]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return prompt\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")  # Replace with your Groq API key\n",
    "\n",
    "synthetic_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for col in columns_to_generate:\n",
    "        text = row[col]\n",
    "        synthetic_text = call_groq_api_for_generation(text, api_key)\n",
    "        synthetic_data.append({col: synthetic_text, 'original_column': col})\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "synthetic_file_path = '../Data/After/synthetic_file.csv'  # Update with your desired file path\n",
    "synthetic_df.to_csv(synthetic_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import requests\n",
    "from groq import Groq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Your message data\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, how are you?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Sanitize the data\n",
    "messages = sanitize_data(messages)\n",
    "\n",
    "# Initialize the client\n",
    "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "# Correct endpoint for chat completions\n",
    "try:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
