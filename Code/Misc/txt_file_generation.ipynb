{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import openai\n",
    "import typing\n",
    "from typing import List, Tuple\n",
    "\n",
    "import tiktoken\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/mnt/data/sample_data.csv')\n",
    "\n",
    "# Combine 'concerns' and 'anything else' columns\n",
    "data['combined_text'] = data['concerns'].fillna('') + ' ' + data['anything else'].fillna('')\n",
    "\n",
    "# Mapping for categories\n",
    "label_map = {'AC': 0, 'TC': 1, 'PC': 2, 'NC': 3}\n",
    "\n",
    "# Apply mapping to the categories\n",
    "data['concerns_category'] = data['concerns_category'].map(label_map)\n",
    "data['anything_else_category'] = data['anything_else_category'].map(label_map)\n",
    "\n",
    "# Final label as the max of both categories (more critical concern)\n",
    "data['label'] = data[['concerns_category', 'anything_else_category']].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read file with encoding: utf-8\n",
      "File successfully read with encoding: latin1\n",
      "                                            concerns concerns category  \\\n",
      "0  That all of my knowledge from calc BC escapes ...                AC   \n",
      "1  My only concern about this course is that I wi...                AC   \n",
      "2  My only concern is if I'll be able to study/pr...                AC   \n",
      "3  My only concern is that so far the video lesso...                AC   \n",
      "4  One thing that was concerning for me last seme...                AC   \n",
      "\n",
      "                                       anything else anything else category  \n",
      "0  The sample exams and quizzes during linear alg...                     AC  \n",
      "1                                                NaN                     NC  \n",
      "2                                                NaN                     NC  \n",
      "3                                                NaN                     NC  \n",
      "4                                                NaN                     NC  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "file_path = 'merged_datafile.csv'\n",
    "\n",
    "# Try different encodings if the default 'utf-8' fails\n",
    "encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"File successfully read with encoding: {encoding}\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read file with encoding: {encoding}\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files created successfully.\n"
     ]
    }
   ],
   "source": [
    "data = df\n",
    "\n",
    "# Step 2: Create dictionaries to store responses for each label\n",
    "responses = {\n",
    "    'AC': [],\n",
    "    'PC': [],\n",
    "    'TC': [],\n",
    "    'NC': []\n",
    "}\n",
    "\n",
    "# Step 3: Extract and group data\n",
    "for _, row in data.iterrows():\n",
    "    concerns_label = row['concerns category']\n",
    "    anything_else_label = row['anything else category']\n",
    "    \n",
    "    if concerns_label in responses:\n",
    "        if pd.notna(row['concerns']):\n",
    "            responses[concerns_label].append(row['concerns'])\n",
    "    if anything_else_label in responses:\n",
    "        if pd.notna(row['anything else']):\n",
    "            responses[anything_else_label].append(row['anything else'])\n",
    "\n",
    "# Step 4: Write responses to text files in list format\n",
    "for label, texts in responses.items():\n",
    "    with open(f'{label}.txt', 'w') as file:\n",
    "        file.write(str(texts))\n",
    "\n",
    "print(\"Text files created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a shuffled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the file content as a string\n",
    "            file_content_str = file.read()\n",
    "\n",
    "            # Safely evaluate the string as a Python literal (list)\n",
    "            file_contents = ast.literal_eval(file_content_str)\n",
    "\n",
    "        return file_contents\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' does not exist.\")\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error while evaluating the file content: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def df_from_file(file_path):\n",
    "    listy = read_list_from_file(file_path)\n",
    " \n",
    "    category = file_path.replace(\".txt\", \"\")\n",
    "    df = pd.DataFrame({'response': listy, 'category': [category] * len(listy)})\n",
    "    return df\n",
    "#Files is a list of files to add it from. Put it all in one dataframe\n",
    "def multiple_df(files):\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        dfs.append(df_from_file(file))\n",
    "    \n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "ret = multiple_df([\"../Data/AC.txt\", \"../Data/PC.txt\", \"../Data/TC.txt\", \"../Data/NC.txt\"])\n",
    "shuffled_df = ret.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
